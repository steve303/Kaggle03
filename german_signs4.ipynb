{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIpaxXnQsvTE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nJmca26iyZET",
    "outputId": "142125bf-8ce0-4bfe-d136-660159fadd08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "wZ59xrUVtEgd",
    "outputId": "96e39766-f95e-46f4-c4c9-89a5357c09ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of X =  3\n",
      "<class 'numpy.ndarray'>\n",
      "(39209, 32, 32, 3)\n",
      "(39209,)\n",
      "(12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#unpack data from source\n",
    "\n",
    "data = np.load('./drive/My Drive/Colab Notebooks/data/csci4622_kaggle3/train_and_test.npz')  #what kind of file is this? zipped np.array file\n",
    "type(data)\n",
    "\n",
    "X = [data[key] for key in data]  #??? why does this work? \"data\" is not a dictionary data type but a NpzFile type\n",
    "\n",
    "print('len of X = ', len(X))  #there are three separate np arrays in the list: 1) train data 2) train labels 3) test data, no labels\n",
    "print(type(X[0]))\n",
    "\n",
    "xtrain0 = X[0]\n",
    "ytrain0 = X[1]  #ytrain0 contains values between 0-42; so 43 categories\n",
    "xtest0 = X[2]   #this is test data for final kaggle submission; it still needs to be normalized\n",
    "\n",
    "print(xtrain0.shape)\n",
    "print(ytrain0.shape)\n",
    "print(xtest0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gorc7EdSaDvE",
    "outputId": "6e50e112-8c84-4a11-fb17-efd17f7a4663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(ytrain0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "fN9GbsqEwgF8",
    "outputId": "8a0d6a44-c7c5-4a2f-b4ed-476799386e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37248, 32, 32, 3)\n",
      "(37248, 43)\n",
      "(1961, 32, 32, 3)\n",
      "(1961, 43)\n"
     ]
    }
   ],
   "source": [
    "#preprocess data - normalize input; divide all data by max pixel value, 255 ; we might also consider z normalization\n",
    "\n",
    "#hot encode the labels\n",
    "ymax = np.max(ytrain0)\n",
    "ytrain = tf.keras.utils.to_categorical(ytrain0, ymax+1)\n",
    "\n",
    "maxpixel_val = np.max(xtrain0) \n",
    "xtrain = xtrain0/maxpixel_val\n",
    "xtest = xtest0/maxpixel_val  \n",
    "\n",
    "x_tr, x_tst, y_tr, y_tst = train_test_split(xtrain, ytrain, test_size = 0.05,shuffle = True)  #results overfit if data is not shuffled!!!\n",
    "print(x_tr.shape)\n",
    "print(y_tr.shape)\n",
    "print(x_tst.shape)\n",
    "print(y_tst.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZhpMj8ZBBwSb",
    "outputId": "cd8b0803-a1c9-4a43-f281-8e486a76c6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(ytrain0[0])  #before and after hot encoding\n",
    "print(ytrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nI4NTgHSMKIT"
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "def build_model(activation, batch_normalization = False):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (3,3), activation = activation, padding = 'same', name = 'conv1', \n",
    "                    input_shape = (32,32,3))) \n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D((2,2), name = 'maxpool1')) \n",
    "    #model.add(Conv2D(16, (3,3), activation = activation, padding = 'same', name = 'conv2' ))\n",
    "    #if batch_normalization: \n",
    "    #    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2,2), name = 'maxpool2'))\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu', padding = 'same', name = 'conv3'))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())\n",
    "    #model.add(MaxPooling2D((2,2), name = 'maxpool3'))\n",
    "    #model.add(Conv2D(32, (3, 3), activation=activation, padding='same', name='conv_4'))\n",
    "    #if batch_normalization: \n",
    "    #    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D((2, 2), name='maxpool_4'))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation=activation, name='dense_1'))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())\n",
    "    #model.add(Dense(32, activation=activation, name='dense_2'))\n",
    "    model.add(Dense(43, activation='softmax', name='output'))  #there are 43 labels\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dTMJVW0oMZzt",
    "outputId": "521c3644-6cc0-4868-8cf7-9df12bfb3d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: 2.7844 - accuracy: 0.3098 - val_loss: 2.0293 - val_accuracy: 0.5074\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 1.7746 - accuracy: 0.5684 - val_loss: 1.4320 - val_accuracy: 0.6629\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 176us/step - loss: 1.3372 - accuracy: 0.6886 - val_loss: 1.1041 - val_accuracy: 0.7542\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 1.0635 - accuracy: 0.7644 - val_loss: 0.9095 - val_accuracy: 0.8006\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.8781 - accuracy: 0.8134 - val_loss: 0.7624 - val_accuracy: 0.8327\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.7422 - accuracy: 0.8484 - val_loss: 0.6562 - val_accuracy: 0.8623\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.6386 - accuracy: 0.8772 - val_loss: 0.5780 - val_accuracy: 0.8812\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.5592 - accuracy: 0.8940 - val_loss: 0.4986 - val_accuracy: 0.8995\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.4943 - accuracy: 0.9091 - val_loss: 0.4388 - val_accuracy: 0.9113\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.4421 - accuracy: 0.9211 - val_loss: 0.4079 - val_accuracy: 0.9235\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.3975 - accuracy: 0.9325 - val_loss: 0.3596 - val_accuracy: 0.9352\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.3605 - accuracy: 0.9397 - val_loss: 0.3326 - val_accuracy: 0.9373\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.3298 - accuracy: 0.9452 - val_loss: 0.3085 - val_accuracy: 0.9429\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.3024 - accuracy: 0.9526 - val_loss: 0.2783 - val_accuracy: 0.9470\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.2782 - accuracy: 0.9558 - val_loss: 0.2625 - val_accuracy: 0.9521\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.2580 - accuracy: 0.9603 - val_loss: 0.2463 - val_accuracy: 0.9551\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.2396 - accuracy: 0.9634 - val_loss: 0.2313 - val_accuracy: 0.9577\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.2253 - accuracy: 0.9660 - val_loss: 0.2156 - val_accuracy: 0.9628\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.2089 - accuracy: 0.9700 - val_loss: 0.2027 - val_accuracy: 0.9607\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.1961 - accuracy: 0.9720 - val_loss: 0.2001 - val_accuracy: 0.9648\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.1855 - accuracy: 0.9742 - val_loss: 0.1845 - val_accuracy: 0.9674\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.1736 - accuracy: 0.9756 - val_loss: 0.1810 - val_accuracy: 0.9709\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.1654 - accuracy: 0.9774 - val_loss: 0.1684 - val_accuracy: 0.9714\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.1562 - accuracy: 0.9793 - val_loss: 0.1585 - val_accuracy: 0.9704\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 175us/step - loss: 0.1480 - accuracy: 0.9807 - val_loss: 0.1645 - val_accuracy: 0.9730\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.1397 - accuracy: 0.9825 - val_loss: 0.1555 - val_accuracy: 0.9740\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.1333 - accuracy: 0.9823 - val_loss: 0.1473 - val_accuracy: 0.9750\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.1296 - accuracy: 0.9832 - val_loss: 0.1378 - val_accuracy: 0.9750\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.1228 - accuracy: 0.9850 - val_loss: 0.1364 - val_accuracy: 0.9750\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.1175 - accuracy: 0.9855 - val_loss: 0.1280 - val_accuracy: 0.9771\n",
      "1961/1961 [==============================] - 0s 98us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.1238 - accuracy: 0.9800 - val_loss: 0.1135 - val_accuracy: 0.9816\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0849 - accuracy: 0.9880 - val_loss: 0.0893 - val_accuracy: 0.9837\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0628 - accuracy: 0.9913 - val_loss: 0.0901 - val_accuracy: 0.9857\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0494 - accuracy: 0.9942 - val_loss: 0.0656 - val_accuracy: 0.9893\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0400 - accuracy: 0.9957 - val_loss: 0.0613 - val_accuracy: 0.9898\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.0334 - accuracy: 0.9966 - val_loss: 0.0539 - val_accuracy: 0.9893\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0273 - accuracy: 0.9978 - val_loss: 0.0478 - val_accuracy: 0.9934\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0243 - accuracy: 0.9981 - val_loss: 0.0485 - val_accuracy: 0.9913\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0219 - accuracy: 0.9980 - val_loss: 0.0448 - val_accuracy: 0.9913\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.0188 - accuracy: 0.9987 - val_loss: 0.0424 - val_accuracy: 0.9934\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0165 - accuracy: 0.9990 - val_loss: 0.0460 - val_accuracy: 0.9913\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0150 - accuracy: 0.9989 - val_loss: 0.0393 - val_accuracy: 0.9944\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0131 - accuracy: 0.9994 - val_loss: 0.0392 - val_accuracy: 0.9934\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0116 - accuracy: 0.9995 - val_loss: 0.0373 - val_accuracy: 0.9949\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0113 - accuracy: 0.9995 - val_loss: 0.0379 - val_accuracy: 0.9929\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0102 - accuracy: 0.9997 - val_loss: 0.0367 - val_accuracy: 0.9934\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0100 - accuracy: 0.9995 - val_loss: 0.0353 - val_accuracy: 0.9939\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0088 - accuracy: 0.9995 - val_loss: 0.0353 - val_accuracy: 0.9949\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 0.0360 - val_accuracy: 0.9934\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0076 - accuracy: 0.9997 - val_loss: 0.0338 - val_accuracy: 0.9954\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0073 - accuracy: 0.9998 - val_loss: 0.0356 - val_accuracy: 0.9934\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0067 - accuracy: 0.9998 - val_loss: 0.0318 - val_accuracy: 0.9944\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0064 - accuracy: 0.9998 - val_loss: 0.0338 - val_accuracy: 0.9934\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 0.0323 - val_accuracy: 0.9944\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.0314 - val_accuracy: 0.9944\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 194us/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.0320 - val_accuracy: 0.9959\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0050 - accuracy: 0.9999 - val_loss: 0.0312 - val_accuracy: 0.9959\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0053 - accuracy: 0.9997 - val_loss: 0.0328 - val_accuracy: 0.9949\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.0374 - val_accuracy: 0.9934\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0049 - accuracy: 0.9998 - val_loss: 0.0307 - val_accuracy: 0.9954\n",
      "1961/1961 [==============================] - 0s 111us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 0.1988 - accuracy: 0.9452 - val_loss: 0.0601 - val_accuracy: 0.9867\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0321 - accuracy: 0.9919 - val_loss: 0.0443 - val_accuracy: 0.9908\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0148 - accuracy: 0.9969 - val_loss: 0.0371 - val_accuracy: 0.9913\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0334 - val_accuracy: 0.9918\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0303 - val_accuracy: 0.9939\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0314 - val_accuracy: 0.9929\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 0.9949\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0284 - val_accuracy: 0.9924\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 0.9939\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.0292 - val_accuracy: 0.9934\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 8.4769e-04 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9934\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 7.2795e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9934\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 6.4562e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9934\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 4.8958e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9939\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 4.7628e-04 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9939\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 4.7445e-04 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9934\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 4.0057e-04 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9939\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 3.8527e-04 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9939\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 3.6614e-04 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 0.9944\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 3.8019e-04 - accuracy: 1.0000 - val_loss: 0.0280 - val_accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 4.5957e-04 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 0.9939\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 3.3091e-04 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 0.9934\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 3.1418e-04 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 0.9934\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 2.8251e-04 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 0.9939\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 2.7246e-04 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 0.9934\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 3.0119e-04 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9934\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 2.4617e-04 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9934\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 2.5247e-04 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9939\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 2.5880e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9934\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 2.3741e-04 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 0.9939\n",
      "1961/1961 [==============================] - 0s 93us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.6844 - accuracy: 0.8062 - val_loss: 0.9122 - val_accuracy: 0.7624\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.2575 - accuracy: 0.9202 - val_loss: 0.1160 - val_accuracy: 0.9679\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0826 - accuracy: 0.9751 - val_loss: 0.0930 - val_accuracy: 0.9745\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 0.0567 - accuracy: 0.9823 - val_loss: 0.0970 - val_accuracy: 0.9740\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0370 - accuracy: 0.9888 - val_loss: 0.0854 - val_accuracy: 0.9791\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.1079 - val_accuracy: 0.9760\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0741 - val_accuracy: 0.9837\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0795 - val_accuracy: 0.9842\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.0896 - val_accuracy: 0.9776\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0548 - val_accuracy: 0.9888\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0890 - val_accuracy: 0.9781\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.0579 - val_accuracy: 0.9862\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0497 - val_accuracy: 0.9913\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0453 - val_accuracy: 0.9903\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0633 - val_accuracy: 0.9867\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0748 - val_accuracy: 0.9867\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.0545 - val_accuracy: 0.9878\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0862 - val_accuracy: 0.9867\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.0869 - val_accuracy: 0.9842\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.1212 - val_accuracy: 0.9730\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0298 - accuracy: 0.9907 - val_loss: 0.1179 - val_accuracy: 0.9750\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: 0.0417 - accuracy: 0.9875 - val_loss: 0.0686 - val_accuracy: 0.9847\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 0.0668 - val_accuracy: 0.9847\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 0.1180 - val_accuracy: 0.9750\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0288 - accuracy: 0.9901 - val_loss: 0.0980 - val_accuracy: 0.9806\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 0.1056 - val_accuracy: 0.9776\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.0872 - val_accuracy: 0.9781\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0221 - accuracy: 0.9926 - val_loss: 0.0912 - val_accuracy: 0.9806\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 0.1092 - val_accuracy: 0.9801\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.1247 - val_accuracy: 0.9745\n",
      "1961/1961 [==============================] - 0s 95us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 198us/step - loss: 0.7302 - accuracy: 0.7818 - val_loss: 1.5171 - val_accuracy: 0.5824\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.8705 - accuracy: 0.7317 - val_loss: 1.0245 - val_accuracy: 0.6996\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 1.1716 - accuracy: 0.6457 - val_loss: 1.3956 - val_accuracy: 0.5686\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 1.2665 - accuracy: 0.6139 - val_loss: 3.2434 - val_accuracy: 0.2565\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 1.6780 - accuracy: 0.5135 - val_loss: 1.9748 - val_accuracy: 0.4263\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 1.4553 - accuracy: 0.5597 - val_loss: 2.1831 - val_accuracy: 0.4141\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 194us/step - loss: 1.5352 - accuracy: 0.5456 - val_loss: 2.2062 - val_accuracy: 0.3549\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 1.6464 - accuracy: 0.5169 - val_loss: 2.3569 - val_accuracy: 0.3559\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 2.0916 - accuracy: 0.4115 - val_loss: 2.1002 - val_accuracy: 0.4018\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 2.0481 - accuracy: 0.4121 - val_loss: 1.4198 - val_accuracy: 0.5339\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 1.9863 - accuracy: 0.4227 - val_loss: 1.3728 - val_accuracy: 0.5890\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 1.6176 - accuracy: 0.5145 - val_loss: 2.2060 - val_accuracy: 0.3870\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 1.4668 - accuracy: 0.5497 - val_loss: 1.6702 - val_accuracy: 0.5038\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 1.6752 - accuracy: 0.5020 - val_loss: 1.9353 - val_accuracy: 0.4513\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 2.2324 - accuracy: 0.3697 - val_loss: 2.0973 - val_accuracy: 0.3998\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 1.7798 - accuracy: 0.4739 - val_loss: 2.5750 - val_accuracy: 0.3111\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 2.3582 - accuracy: 0.3443 - val_loss: 2.4757 - val_accuracy: 0.3157\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 1.5369 - accuracy: 0.5325 - val_loss: 1.2390 - val_accuracy: 0.6012\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 1.7477 - accuracy: 0.4856 - val_loss: 1.4174 - val_accuracy: 0.5681\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 1.7534 - accuracy: 0.4830 - val_loss: 1.7215 - val_accuracy: 0.4885\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 2.1482 - accuracy: 0.3879 - val_loss: 2.0801 - val_accuracy: 0.4161\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 2.0502 - accuracy: 0.4111 - val_loss: 1.7235 - val_accuracy: 0.4788\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 1.9956 - accuracy: 0.4222 - val_loss: 2.4023 - val_accuracy: 0.3182\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 2.2179 - accuracy: 0.3780 - val_loss: 2.0199 - val_accuracy: 0.4197\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 2.4910 - accuracy: 0.3087 - val_loss: 2.8261 - val_accuracy: 0.2279\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 2.1139 - accuracy: 0.3899 - val_loss: 1.5072 - val_accuracy: 0.5309\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 2.4570 - accuracy: 0.3177 - val_loss: 2.2403 - val_accuracy: 0.3610\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 2.0884 - accuracy: 0.3971 - val_loss: 2.0597 - val_accuracy: 0.4074\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 2.5049 - accuracy: 0.3014 - val_loss: 1.9939 - val_accuracy: 0.4233\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 2.0043 - accuracy: 0.4200 - val_loss: 2.1374 - val_accuracy: 0.3845\n",
      "1961/1961 [==============================] - 0s 106us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: nan - accuracy: 0.0070 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 198us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "1961/1961 [==============================] - 0s 109us/step\n"
     ]
    }
   ],
   "source": [
    "#increment learning rate with sigmoid function\n",
    "model = build_model(activation = 'sigmoid', batch_normalization = True)\n",
    "learn = [0.001, 0.01, 0.10, 0.50, 1, 5]\n",
    "res_sig =[]\n",
    "\n",
    "for i in learn:\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate = i, momentum = 0.0, nesterov=False)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    model.fit(x_tr, y_tr, epochs = 30, validation_data= (x_tst, y_tst))\n",
    "    loss, accuracy = model.evaluate(x_tst, y_tst)\n",
    "    res_sig.append(accuracy)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A_VoEctKH0qs",
    "outputId": "84abe8a8-cd1d-4eba-b68c-aee09c1be97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: 2.8671 - accuracy: 0.2865 - val_loss: 2.1547 - val_accuracy: 0.4916\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 194us/step - loss: 1.8968 - accuracy: 0.5402 - val_loss: 1.5885 - val_accuracy: 0.6242\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 1.4717 - accuracy: 0.6536 - val_loss: 1.2575 - val_accuracy: 0.6986\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 1.1907 - accuracy: 0.7335 - val_loss: 1.0420 - val_accuracy: 0.7649\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.9934 - accuracy: 0.7873 - val_loss: 0.8727 - val_accuracy: 0.8062\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.8433 - accuracy: 0.8257 - val_loss: 0.7488 - val_accuracy: 0.8465\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.7286 - accuracy: 0.8549 - val_loss: 0.6634 - val_accuracy: 0.8705\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.6341 - accuracy: 0.8787 - val_loss: 0.5972 - val_accuracy: 0.8868\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.5619 - accuracy: 0.8937 - val_loss: 0.5010 - val_accuracy: 0.9021\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.4998 - accuracy: 0.9090 - val_loss: 0.4614 - val_accuracy: 0.9174\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.4483 - accuracy: 0.9192 - val_loss: 0.4140 - val_accuracy: 0.9250\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.4058 - accuracy: 0.9304 - val_loss: 0.3844 - val_accuracy: 0.9312\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.3693 - accuracy: 0.9378 - val_loss: 0.3496 - val_accuracy: 0.9352\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.3376 - accuracy: 0.9443 - val_loss: 0.3158 - val_accuracy: 0.9454\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: 0.3123 - accuracy: 0.9473 - val_loss: 0.2829 - val_accuracy: 0.9516\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.2883 - accuracy: 0.9530 - val_loss: 0.2665 - val_accuracy: 0.9556\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.2680 - accuracy: 0.9564 - val_loss: 0.2549 - val_accuracy: 0.9561\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 198us/step - loss: 0.2474 - accuracy: 0.9609 - val_loss: 0.2378 - val_accuracy: 0.9607\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.2315 - accuracy: 0.9650 - val_loss: 0.2288 - val_accuracy: 0.9618\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: 0.2176 - accuracy: 0.9667 - val_loss: 0.2118 - val_accuracy: 0.9623\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.2038 - accuracy: 0.9699 - val_loss: 0.1971 - val_accuracy: 0.9653\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 0.1910 - accuracy: 0.9709 - val_loss: 0.1904 - val_accuracy: 0.9699\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: 0.1812 - accuracy: 0.9731 - val_loss: 0.1789 - val_accuracy: 0.9669\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 8s 202us/step - loss: 0.1707 - accuracy: 0.9748 - val_loss: 0.1780 - val_accuracy: 0.9699\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 0.1632 - accuracy: 0.9771 - val_loss: 0.1642 - val_accuracy: 0.9725\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.1525 - accuracy: 0.9787 - val_loss: 0.1577 - val_accuracy: 0.9720\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.1463 - accuracy: 0.9793 - val_loss: 0.1562 - val_accuracy: 0.9750\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.1406 - accuracy: 0.9798 - val_loss: 0.1514 - val_accuracy: 0.9760\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.1347 - accuracy: 0.9807 - val_loss: 0.1445 - val_accuracy: 0.9760\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.1276 - accuracy: 0.9828 - val_loss: 0.1411 - val_accuracy: 0.9765\n",
      "1961/1961 [==============================] - 0s 108us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 8s 205us/step - loss: 0.1490 - accuracy: 0.9731 - val_loss: 0.1571 - val_accuracy: 0.9714\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.1014 - accuracy: 0.9827 - val_loss: 0.0942 - val_accuracy: 0.9811\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 0.0735 - accuracy: 0.9886 - val_loss: 0.0810 - val_accuracy: 0.9816\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0587 - accuracy: 0.9912 - val_loss: 0.0766 - val_accuracy: 0.9847\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.0460 - accuracy: 0.9940 - val_loss: 0.0631 - val_accuracy: 0.9888\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0390 - accuracy: 0.9950 - val_loss: 0.0661 - val_accuracy: 0.9857\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0326 - accuracy: 0.9963 - val_loss: 0.0690 - val_accuracy: 0.9847\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0281 - accuracy: 0.9970 - val_loss: 0.0531 - val_accuracy: 0.9903\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0234 - accuracy: 0.9982 - val_loss: 0.0482 - val_accuracy: 0.9908\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0213 - accuracy: 0.9980 - val_loss: 0.0426 - val_accuracy: 0.9908\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0197 - accuracy: 0.9980 - val_loss: 0.0483 - val_accuracy: 0.9903\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0185 - accuracy: 0.9981 - val_loss: 0.0419 - val_accuracy: 0.9898\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0150 - accuracy: 0.9987 - val_loss: 0.0430 - val_accuracy: 0.9913\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0143 - accuracy: 0.9989 - val_loss: 0.0392 - val_accuracy: 0.9918\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0123 - accuracy: 0.9993 - val_loss: 0.0385 - val_accuracy: 0.9913\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0120 - accuracy: 0.9990 - val_loss: 0.0434 - val_accuracy: 0.9903\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.0381 - val_accuracy: 0.9918\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0105 - accuracy: 0.9994 - val_loss: 0.0383 - val_accuracy: 0.9918\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0103 - accuracy: 0.9991 - val_loss: 0.0366 - val_accuracy: 0.9908\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0094 - accuracy: 0.9994 - val_loss: 0.0406 - val_accuracy: 0.9913\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.0361 - val_accuracy: 0.9903\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0081 - accuracy: 0.9994 - val_loss: 0.0328 - val_accuracy: 0.9929\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0074 - accuracy: 0.9996 - val_loss: 0.0357 - val_accuracy: 0.9918\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0069 - accuracy: 0.9997 - val_loss: 0.0361 - val_accuracy: 0.9924\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 0.0341 - val_accuracy: 0.9929\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0071 - accuracy: 0.9994 - val_loss: 0.0369 - val_accuracy: 0.9929\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0067 - accuracy: 0.9995 - val_loss: 0.0351 - val_accuracy: 0.9929\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0055 - accuracy: 0.9999 - val_loss: 0.0347 - val_accuracy: 0.9924\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0058 - accuracy: 0.9998 - val_loss: 0.0359 - val_accuracy: 0.9918\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0053 - accuracy: 0.9998 - val_loss: 0.0357 - val_accuracy: 0.9913\n",
      "1961/1961 [==============================] - 0s 95us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.3839 - accuracy: 0.8980 - val_loss: 0.2006 - val_accuracy: 0.9419\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.1113 - accuracy: 0.9691 - val_loss: 0.0929 - val_accuracy: 0.9740\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0621 - accuracy: 0.9826 - val_loss: 0.1638 - val_accuracy: 0.9526\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0411 - accuracy: 0.9882 - val_loss: 0.1030 - val_accuracy: 0.9842\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0239 - accuracy: 0.9941 - val_loss: 0.0573 - val_accuracy: 0.9867\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.0581 - val_accuracy: 0.9852\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0122 - accuracy: 0.9974 - val_loss: 0.0551 - val_accuracy: 0.9847\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0685 - accuracy: 0.9804 - val_loss: 0.0904 - val_accuracy: 0.9781\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0397 - accuracy: 0.9880 - val_loss: 0.0892 - val_accuracy: 0.9755\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0233 - accuracy: 0.9935 - val_loss: 0.0615 - val_accuracy: 0.9832\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0163 - accuracy: 0.9958 - val_loss: 0.0509 - val_accuracy: 0.9857\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 176us/step - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.0415 - val_accuracy: 0.9893\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0463 - val_accuracy: 0.9878\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.0073 - accuracy: 0.9984 - val_loss: 0.0438 - val_accuracy: 0.9883\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.1868 - val_accuracy: 0.9806\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 176us/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0344 - val_accuracy: 0.9924\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0400 - val_accuracy: 0.9898\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0351 - val_accuracy: 0.9924\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 6s 174us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0391 - val_accuracy: 0.9908\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0317 - val_accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0424 - val_accuracy: 0.9883\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0310 - val_accuracy: 0.9913\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0430 - val_accuracy: 0.9913\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0333 - val_accuracy: 0.9918\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0347 - val_accuracy: 0.9913\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0319 - val_accuracy: 0.9929\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0340 - val_accuracy: 0.9913\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0347 - val_accuracy: 0.9929\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 8.8546e-04 - accuracy: 0.9999 - val_loss: 0.0353 - val_accuracy: 0.9918\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0407 - val_accuracy: 0.9903\n",
      "1961/1961 [==============================] - 0s 96us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 198us/step - loss: 0.5936 - accuracy: 0.8341 - val_loss: 0.3195 - val_accuracy: 0.8980\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 199us/step - loss: 0.1654 - accuracy: 0.9504 - val_loss: 0.1529 - val_accuracy: 0.9541\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0988 - accuracy: 0.9696 - val_loss: 0.1435 - val_accuracy: 0.9551\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0663 - accuracy: 0.9790 - val_loss: 0.1198 - val_accuracy: 0.9699\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0524 - accuracy: 0.9834 - val_loss: 0.1170 - val_accuracy: 0.9699\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0441 - accuracy: 0.9860 - val_loss: 0.1893 - val_accuracy: 0.9709\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.0397 - accuracy: 0.9871 - val_loss: 0.1301 - val_accuracy: 0.9776\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.0779 - val_accuracy: 0.9842\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0248 - accuracy: 0.9922 - val_loss: 0.0772 - val_accuracy: 0.9822\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 176us/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0895 - val_accuracy: 0.9796\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.1011 - val_accuracy: 0.9837\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0246 - accuracy: 0.9922 - val_loss: 0.0605 - val_accuracy: 0.9888\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.0734 - val_accuracy: 0.9816\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.0710 - val_accuracy: 0.9862\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0960 - val_accuracy: 0.9801\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.0783 - val_accuracy: 0.9883\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.0798 - val_accuracy: 0.9842\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0785 - val_accuracy: 0.9816\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0714 - val_accuracy: 0.9857\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0650 - val_accuracy: 0.9873\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0701 - val_accuracy: 0.9878\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0740 - val_accuracy: 0.9862\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.0896 - val_accuracy: 0.9883\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.0658 - val_accuracy: 0.9878\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.0729 - val_accuracy: 0.9873\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0526 - accuracy: 0.9899 - val_loss: 0.1167 - val_accuracy: 0.9730\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0172 - accuracy: 0.9943 - val_loss: 0.0888 - val_accuracy: 0.9811\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0628 - val_accuracy: 0.9883\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0759 - val_accuracy: 0.9873\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0846 - val_accuracy: 0.9857\n",
      "1961/1961 [==============================] - 0s 103us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: 1.0064 - accuracy: 0.7480 - val_loss: 0.3509 - val_accuracy: 0.9001\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.2043 - accuracy: 0.9379 - val_loss: 0.1375 - val_accuracy: 0.9592\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.1046 - accuracy: 0.9659 - val_loss: 0.1785 - val_accuracy: 0.9516\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0689 - accuracy: 0.9792 - val_loss: 0.1023 - val_accuracy: 0.9750\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 175us/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 0.0863 - val_accuracy: 0.9781\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.0933 - val_accuracy: 0.9786\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.1179 - val_accuracy: 0.9735\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0307 - accuracy: 0.9905 - val_loss: 0.0921 - val_accuracy: 0.9781\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0819 - val_accuracy: 0.9791\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.0702 - val_accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0614 - val_accuracy: 0.9873\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0981 - accuracy: 0.9814 - val_loss: 0.0672 - val_accuracy: 0.9801\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0137 - accuracy: 0.9961 - val_loss: 0.0704 - val_accuracy: 0.9842\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 0.0631 - val_accuracy: 0.9862\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 0.0780 - val_accuracy: 0.9857\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0582 - val_accuracy: 0.9888\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0684 - val_accuracy: 0.9852\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0600 - val_accuracy: 0.9893\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0732 - val_accuracy: 0.9878\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0771 - val_accuracy: 0.9842\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 179us/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.0894 - val_accuracy: 0.9867\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.0884 - val_accuracy: 0.9857\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0838 - val_accuracy: 0.9857\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0717 - val_accuracy: 0.9893\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0669 - val_accuracy: 0.9878\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0696 - val_accuracy: 0.9873\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0669 - val_accuracy: 0.9883\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0883 - val_accuracy: 0.9852\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0953 - val_accuracy: 0.9847\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0909 - val_accuracy: 0.9862\n",
      "1961/1961 [==============================] - 0s 109us/step\n",
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 198us/step - loss: nan - accuracy: 0.0088 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 190us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 196us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 189us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 188us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 200us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: nan - accuracy: 0.0052 - val_loss: nan - val_accuracy: 0.0082\n",
      "1961/1961 [==============================] - 0s 100us/step\n"
     ]
    }
   ],
   "source": [
    "#increment learning rate with relu\n",
    "model = build_model(activation = 'relu', batch_normalization = True)\n",
    "learn = [0.001, 0.01, 0.10, 0.50, 1, 5]\n",
    "res_relu =[]\n",
    "\n",
    "for i in learn:\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate = i, momentum = 0.0, nesterov=False)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    model.fit(x_tr, y_tr, epochs = 30, validation_data= (x_tst, y_tst))\n",
    "    loss, accuracy = model.evaluate(x_tst, y_tst)\n",
    "    res_relu.append(accuracy)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "wEMCwZ8VFeOH",
    "outputId": "7f9f1bb0-e5fe-4bc5-d04d-035560b8653f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn rate:  [0.001, 0.01, 0.1, 0.5, 1, 5]\n",
      "relu:  [0.976542592048645, 0.9913309812545776, 0.9903110861778259, 0.9857215881347656, 0.9862315058708191, 0.008159102872014046]\n",
      "sigmoid:  [0.9770525097846985, 0.9954105019569397, 0.9938806891441345, 0.9745028018951416, 0.3844977021217346, 0.008159102872014046]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot learning rates vs accuracy\n",
    "#res_sig = np.array(res)\n",
    "plt.plot(learn, res_sig, label='sigmoid')\n",
    "plt.plot(learn, res_relu, label='relu')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Rate')\n",
    "print('learn rate: ', learn)\n",
    "print('relu: ',res_relu)\n",
    "print('sigmoid: ', res_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vp-CKtK_Mbn4",
    "outputId": "d24bb243-65a6-4e36-aa98-b68e77fdef8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37248 samples, validate on 1961 samples\n",
      "Epoch 1/30\n",
      "37248/37248 [==============================] - 7s 197us/step - loss: 0.9269 - accuracy: 0.7779 - val_loss: 0.2489 - val_accuracy: 0.9434\n",
      "Epoch 2/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.1881 - accuracy: 0.9640 - val_loss: 0.1438 - val_accuracy: 0.9694\n",
      "Epoch 3/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0944 - accuracy: 0.9837 - val_loss: 0.1295 - val_accuracy: 0.9648\n",
      "Epoch 4/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0563 - accuracy: 0.9922 - val_loss: 0.0762 - val_accuracy: 0.9796\n",
      "Epoch 5/30\n",
      "37248/37248 [==============================] - 7s 191us/step - loss: 0.0400 - accuracy: 0.9950 - val_loss: 0.0569 - val_accuracy: 0.9867\n",
      "Epoch 6/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 0.0289 - accuracy: 0.9966 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
      "Epoch 7/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0241 - accuracy: 0.9970 - val_loss: 0.0484 - val_accuracy: 0.9852\n",
      "Epoch 8/30\n",
      "37248/37248 [==============================] - 7s 181us/step - loss: 0.0184 - accuracy: 0.9981 - val_loss: 0.0387 - val_accuracy: 0.9903\n",
      "Epoch 9/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0147 - accuracy: 0.9990 - val_loss: 0.0417 - val_accuracy: 0.9883\n",
      "Epoch 10/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.0129 - accuracy: 0.9990 - val_loss: 0.0373 - val_accuracy: 0.9888\n",
      "Epoch 11/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0113 - accuracy: 0.9992 - val_loss: 0.0396 - val_accuracy: 0.9888\n",
      "Epoch 12/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0093 - accuracy: 0.9993 - val_loss: 0.0331 - val_accuracy: 0.9898\n",
      "Epoch 13/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0090 - accuracy: 0.9991 - val_loss: 0.0471 - val_accuracy: 0.9862\n",
      "Epoch 14/30\n",
      "37248/37248 [==============================] - 7s 192us/step - loss: 0.0078 - accuracy: 0.9994 - val_loss: 0.0326 - val_accuracy: 0.9898\n",
      "Epoch 15/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0064 - accuracy: 0.9997 - val_loss: 0.0352 - val_accuracy: 0.9908\n",
      "Epoch 16/30\n",
      "37248/37248 [==============================] - 7s 195us/step - loss: 0.0064 - accuracy: 0.9995 - val_loss: 0.0321 - val_accuracy: 0.9918\n",
      "Epoch 17/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 0.0322 - val_accuracy: 0.9898\n",
      "Epoch 18/30\n",
      "37248/37248 [==============================] - 7s 180us/step - loss: 0.0049 - accuracy: 0.9999 - val_loss: 0.0304 - val_accuracy: 0.9918\n",
      "Epoch 19/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0046 - accuracy: 0.9998 - val_loss: 0.0291 - val_accuracy: 0.9913\n",
      "Epoch 20/30\n",
      "37248/37248 [==============================] - 7s 185us/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 0.0254 - val_accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.0291 - val_accuracy: 0.9924\n",
      "Epoch 22/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0035 - accuracy: 0.9999 - val_loss: 0.0260 - val_accuracy: 0.9929\n",
      "Epoch 23/30\n",
      "37248/37248 [==============================] - 7s 178us/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.0279 - val_accuracy: 0.9918\n",
      "Epoch 24/30\n",
      "37248/37248 [==============================] - 7s 187us/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0263 - val_accuracy: 0.9929\n",
      "Epoch 25/30\n",
      "37248/37248 [==============================] - 7s 184us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 0.9918\n",
      "Epoch 26/30\n",
      "37248/37248 [==============================] - 7s 186us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 0.9929\n",
      "Epoch 27/30\n",
      "37248/37248 [==============================] - 7s 182us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 0.9913\n",
      "Epoch 28/30\n",
      "37248/37248 [==============================] - 7s 183us/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0256 - val_accuracy: 0.9939\n",
      "Epoch 29/30\n",
      "37248/37248 [==============================] - 7s 177us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 0.9934\n",
      "Epoch 30/30\n",
      "37248/37248 [==============================] - 7s 193us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f1903200c88>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run final model for submission\n",
    "model = build_model(activation = 'relu', batch_normalization = True)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 0.01, momentum = 0.5, nesterov=True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "model.fit(x_tr, y_tr, epochs = 30, validation_data= (x_tst, y_tst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "oEVXvN8yf158",
    "outputId": "280aaee3-fe29-47fd-c950-701b7978da8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 0s 95us/step\n",
      "Test accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_tst, y_tst)\n",
    "print('Test accuracy: %.4f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRf_XhQeldqz"
   },
   "source": [
    "##below is to create the kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x40aYdvy9Gik"
   },
   "outputs": [],
   "source": [
    "yhat_hot = model.predict_classes(xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7SOv9A129bMz",
    "outputId": "963e204f-6868-4989-fd4e-64d1adf743bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12630,)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gCYcoFXgGcyx",
    "outputId": "f3a64451-869c-439a-897a-e6f66cbc629d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  1, 38, 33, 11])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_hot[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wvDAslmM_P8W",
    "outputId": "f1821ecc-0156-409f-8e86-1371f6ec411b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh_42'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "for i in range(len(set(ytrain0))):\n",
    "    a = 'oh_' + str(i)\n",
    "    label_list.append(a)\n",
    "\n",
    "label_list[-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xlL_F0PzB637",
    "outputId": "f4a4a0a5-1732-4f89-858c-52edf890faff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12630, 43)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = len(yhat_hot)\n",
    "columns = len(set(ytrain0))\n",
    "res_array = np.zeros((rows, columns), dtype=int)\n",
    "res_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Py_rjzlEIzO"
   },
   "outputs": [],
   "source": [
    "df_hot = pd.DataFrame(res_array, columns = label_list)\n",
    "i = 0\n",
    "for lb in yhat_hot:\n",
    "    df_hot.iloc[i][lb] = 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cB8jkS0fGmrW"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_hot.insert(0,'id', np.arange(len(xtest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "lRdjtVxmJlIh",
    "outputId": "11a4aed3-a269-4cae-ed14-37c1c5d9cf38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>oh_0</th>\n",
       "      <th>oh_1</th>\n",
       "      <th>oh_2</th>\n",
       "      <th>oh_3</th>\n",
       "      <th>oh_4</th>\n",
       "      <th>oh_5</th>\n",
       "      <th>oh_6</th>\n",
       "      <th>oh_7</th>\n",
       "      <th>oh_8</th>\n",
       "      <th>oh_9</th>\n",
       "      <th>oh_10</th>\n",
       "      <th>oh_11</th>\n",
       "      <th>oh_12</th>\n",
       "      <th>oh_13</th>\n",
       "      <th>oh_14</th>\n",
       "      <th>oh_15</th>\n",
       "      <th>oh_16</th>\n",
       "      <th>oh_17</th>\n",
       "      <th>oh_18</th>\n",
       "      <th>oh_19</th>\n",
       "      <th>oh_20</th>\n",
       "      <th>oh_21</th>\n",
       "      <th>oh_22</th>\n",
       "      <th>oh_23</th>\n",
       "      <th>oh_24</th>\n",
       "      <th>oh_25</th>\n",
       "      <th>oh_26</th>\n",
       "      <th>oh_27</th>\n",
       "      <th>oh_28</th>\n",
       "      <th>oh_29</th>\n",
       "      <th>oh_30</th>\n",
       "      <th>oh_31</th>\n",
       "      <th>oh_32</th>\n",
       "      <th>oh_33</th>\n",
       "      <th>oh_34</th>\n",
       "      <th>oh_35</th>\n",
       "      <th>oh_36</th>\n",
       "      <th>oh_37</th>\n",
       "      <th>oh_38</th>\n",
       "      <th>oh_39</th>\n",
       "      <th>oh_40</th>\n",
       "      <th>oh_41</th>\n",
       "      <th>oh_42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  oh_0  oh_1  oh_2  oh_3  oh_4  ...  oh_37  oh_38  oh_39  oh_40  oh_41  oh_42\n",
       "0   0     0     0     0     0     0  ...      0      0      0      0      0      0\n",
       "1   1     0     1     0     0     0  ...      0      0      0      0      0      0\n",
       "2   2     0     0     0     0     0  ...      0      1      0      0      0      0\n",
       "3   3     0     0     0     0     0  ...      0      0      0      0      0      0\n",
       "4   4     0     0     0     0     0  ...      0      0      0      0      0      0\n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqAmCzPQJwlw"
   },
   "outputs": [],
   "source": [
    "df_hot.to_csv('./drive/My Drive/Colab Notebooks/data/csci4622_kaggle3/final_hot.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ro_md9qLudu"
   },
   "source": [
    "##below is to error check and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2woLArmNjpdJ",
    "outputId": "2f5b3a55-7492-44d2-baa4-80715a48b3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961/1961 [==============================] - 0s 91us/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_tst, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULOq39_kgeg"
   },
   "outputs": [],
   "source": [
    "y_TST = [np.argmax(y_tst[i]) for i in range(len(y_tst))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ERU5jH6xpWxR",
    "outputId": "81a35ee6-7360-4325-8129-399a1f9387db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 2, 3, 38, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_TST[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "Ua3YOLX731Dz",
    "outputId": "64e3fb6f-8369-4b68-9222-b6ed70892da4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst[0:5]  #y_TST and y_tst both check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxSJPt-en2Br"
   },
   "outputs": [],
   "source": [
    "y_HAT = [np.argmax(yhat[i]) for i in range(len(yhat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1_NiK8FLtQEy",
    "outputId": "d64c1e8f-961a-4473-9de7-716428eca15e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 2, 3, 38, 4]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_HAT[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "colab_type": "code",
    "id": "Csy-LHTv0vz9",
    "outputId": "4757dda5-2f7d-46a6-aca8-d2aeba15567c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 16., 108., 105.,  69.,  98.,  95.,  19.,  75.,  65.,  68., 100.,\n",
       "         63., 112., 122.,  43.,  27.,  21.,  70.,  60.,  11.,  14.,  21.,\n",
       "         10.,  24.,  11.,  72.,  23.,  10.,  19.,  11.,  31.,  27.,   5.,\n",
       "         32.,  31.,  57.,  16.,   8., 114.,  24.,  19.,  17.,  18.]),\n",
       " array([ 0.        ,  0.97674419,  1.95348837,  2.93023256,  3.90697674,\n",
       "         4.88372093,  5.86046512,  6.8372093 ,  7.81395349,  8.79069767,\n",
       "         9.76744186, 10.74418605, 11.72093023, 12.69767442, 13.6744186 ,\n",
       "        14.65116279, 15.62790698, 16.60465116, 17.58139535, 18.55813953,\n",
       "        19.53488372, 20.51162791, 21.48837209, 22.46511628, 23.44186047,\n",
       "        24.41860465, 25.39534884, 26.37209302, 27.34883721, 28.3255814 ,\n",
       "        29.30232558, 30.27906977, 31.25581395, 32.23255814, 33.20930233,\n",
       "        34.18604651, 35.1627907 , 36.13953488, 37.11627907, 38.09302326,\n",
       "        39.06976744, 40.04651163, 41.02325581, 42.        ]),\n",
       " <a list of 43 Patch objects>)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.hist(y_HAT, color = 'r', alpha = 0.9, bins = 43)\n",
    "plt.hist(y_TST, color='b', alpha = 0.4, bins = 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeYCkHs_Mbui"
   },
   "source": [
    "missclassification can be seen in the non overlapping areas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzFSSeI8M74_"
   },
   "source": [
    "###code below not used - it is to see probabilities from the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HBHR9rlth8jg",
    "outputId": "ef998b89-5cc4-4ae0-dcf3-8207f7a5b7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 1s 85us/step\n"
     ]
    }
   ],
   "source": [
    "yhat_final = model.predict(xtest, verbose = True)\n",
    "y_HAT_final = [np.argmax(yhat_final[i]) for i in range(len(yhat_final))]\n",
    "y_HAT_finalP = [np.max(yhat_final[i]) for i in range(len(yhat_final))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "rfth8k3xjxYM",
    "outputId": "7f7873db-bab2-43d6-d9df-471547f54070"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.88063336e-07, 1.00801534e-09, 6.77827499e-12, 5.79216730e-10,\n",
       "        5.89931792e-09, 1.97034879e-05, 5.83450728e-08, 4.74472927e-06,\n",
       "        6.25310804e-07, 8.04740921e-07, 1.15388701e-07, 2.89267348e-08,\n",
       "        2.01572026e-09, 9.41273587e-11, 7.30854310e-08, 1.86511409e-06,\n",
       "        9.99969482e-01, 1.70704375e-08, 4.91471326e-08, 1.90370372e-10,\n",
       "        1.78169639e-08, 1.49387887e-07, 4.13809232e-07, 1.07968155e-07,\n",
       "        8.99692487e-10, 6.79985457e-10, 4.17013002e-09, 1.51781933e-07,\n",
       "        4.15909867e-08, 2.38604400e-07, 1.05315454e-08, 7.38812460e-08,\n",
       "        2.51790517e-08, 4.91287366e-09, 1.61741065e-09, 5.34045741e-09,\n",
       "        8.14438916e-09, 1.98756034e-09, 2.30225750e-09, 8.90123752e-10,\n",
       "        2.94837804e-07, 4.14179965e-07, 3.45475506e-08],\n",
       "       [3.13379864e-08, 1.00000000e+00, 1.43865908e-09, 5.86136565e-12,\n",
       "        3.35849890e-08, 6.23317975e-09, 1.60993163e-09, 2.44177559e-14,\n",
       "        1.33954675e-12, 3.67512095e-15, 1.10965089e-12, 1.73174328e-13,\n",
       "        1.56149467e-13, 6.25058772e-12, 1.98627746e-11, 9.01621087e-15,\n",
       "        8.93301167e-13, 1.39997443e-12, 5.96239413e-09, 1.01488934e-12,\n",
       "        1.04129780e-12, 1.26971758e-11, 3.07948545e-13, 2.54255102e-12,\n",
       "        1.90425835e-11, 5.42659797e-12, 2.36940762e-12, 2.23245658e-11,\n",
       "        8.45666581e-14, 7.01621649e-13, 2.45445182e-15, 1.82227271e-11,\n",
       "        6.95970110e-14, 5.50274453e-13, 1.65435211e-13, 3.17065818e-13,\n",
       "        4.92423637e-12, 1.23491070e-11, 2.20034962e-11, 7.09242506e-13,\n",
       "        7.48986539e-13, 2.12012677e-12, 1.26149568e-11]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_final[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uPMP-Lflj1bT",
    "outputId": "9429b42a-3beb-42d5-9fb5-5e43e986465e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 1, 38, 33, 11]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_HAT_final[0:5]  #predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aG7SRArXVDDs",
    "outputId": "c193fa61-bd59-401e-f1de-e6fb41635118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999695, 1.0, 0.9999999, 0.99967444, 0.99999976]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_HAT_finalP[0:5]  #predicted probabilities"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "german_signs4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
